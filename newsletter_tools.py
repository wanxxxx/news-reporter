import os
import json
import re
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional

import feedparser
import requests
from bs4 import BeautifulSoup
import trafilatura
from openai import OpenAI
from dotenv import load_dotenv

import lark_oapi as lark
from lark_oapi.api.docx.v1 import (
    CreateDocumentRequest,
    CreateDocumentRequestBody,
    ConvertDocumentRequest,
    ConvertDocumentRequestBody,
    TextElement,
    TextRun,
    TextElementStyle,
    Link,
    UpdateBlockRequest,
    BatchUpdateDocumentBlockRequest,
    BatchUpdateDocumentBlockRequestBody,
    CreateDocumentBlockChildrenRequest,
    CreateDocumentBlockChildrenRequestBody,
    Block,
    Text as TextModel
)
from lark_oapi.api.im.v1 import CreateMessageRequest, CreateMessageRequestBody
from lark_oapi.api.drive.v1 import (
    CreatePermissionMemberRequest,
    BaseMember,
    BatchCreatePermissionMemberRequest,
    BatchCreatePermissionMemberRequestBody
)

load_dotenv()

client = lark.Client.builder() \
    .app_id(os.getenv("FEISHU_APP_ID")) \
    .app_secret(os.getenv("FEISHU_APP_SECRET")) \
    .log_level(lark.LogLevel.INFO) \
    .build()


TARGET_SITES = [
    'https://explorersweb.com/',
    'https://www.outsideonline.com/home',
    'https://www.climbing.com/',
    'https://publications.americanalpineclub.org/',
    'https://gripped.com/'
]

RSS_FEEDS = {
    'https://explorersweb.com/': 'https://explorersweb.com/feed/',
    'https://www.outsideonline.com/home': 'https://www.outsideonline.com/feed',
    'https://www.climbing.com/': 'https://www.climbing.com/feed/',
    'https://publications.americanalpineclub.org/': None,
    'https://gripped.com/': 'https://gripped.com/feed/'
}


def fetch_outdoor_articles(start_date: date, end_date: date) -> List[Dict]:
    articles = []
    
    for site_url in TARGET_SITES:
        rss_feed = RSS_FEEDS.get(site_url)
        
        if rss_feed:
            articles.extend(_fetch_from_rss(rss_feed, site_url, start_date, end_date))
        else:
            articles.extend(_fetch_from_html(site_url, start_date, end_date))
    
    return articles


def _fetch_from_rss(rss_url: str, site_url: str, start_date: date, end_date: date) -> List[Dict]:
    articles = []
    
    try:
        print(f"\nğŸ” æ­£åœ¨è§£æ RSS: {rss_url}")
        feed = feedparser.parse(rss_url)
        print(f"   RSS feed ä¸­å…±æœ‰ {len(feed.entries)} æ¡ç›®")
        
        for entry in feed.entries:
            if hasattr(entry, 'published_parsed'):
                article_date = datetime(*entry.published_parsed[:6])
                title = entry.get('title', '')
                
                print(f"   æ£€æŸ¥æ–‡ç« : {title}")
                print(f"      æ–‡ç« æ—¥æœŸ: {article_date.date()}")
                print(f"      ç›®æ ‡èŒƒå›´: {start_date} åˆ° {end_date}")
                
                if start_date <= article_date.date() <= end_date:
                    # æ–‡ç« æ—¥æœŸåœ¨èŒƒå›´å†…ï¼Œå¼€å§‹å¤„ç†
                    article_url = entry.get('link', '')
                    
                    print(f"ğŸ“… æ‰¾åˆ°ç¬¦åˆæ—¥æœŸçš„æ–‡ç« : {title}")
                    print(f"   æ—¥æœŸ: {article_date}")
                    print(f"   é“¾æ¥: {article_url}")
                    
                    # æå–æ–‡ç« å†…å®¹
                    content_text = _extract_content(article_url)
                    
                    if content_text:
                        articles.append({
                            'site': site_url,
                            'url': article_url,
                            'title': title,
                            'date': article_date.date().isoformat(),
                            'content_text': content_text
                        })
    except Exception as e:
        pass
    
    return articles


def _fetch_from_html(site_url: str, start_date: date, end_date: date) -> List[Dict]:
    articles = []
    
    try:
        response = requests.get(site_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        article_links = _extract_article_links(soup, site_url)
        
        for link in article_links:
            content_text = _extract_content(link)
            
            if content_text:
                articles.append({
                    'site': site_url,
                    'url': link,
                    'title': _extract_title_from_url(link),
                    'content_text': content_text
                })
    except Exception as e:
        pass
    
    return articles


def _extract_article_links(soup: BeautifulSoup, base_url: str) -> List[str]:
    links = []
    
    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']
        
        if href.startswith('/'):
            href = base_url.rstrip('/') + href
        elif not href.startswith('http'):
            continue
        
        if _is_article_link(href):
            links.append(href)
    
    return list(set(links))


def _is_article_link(url: str) -> bool:
    exclude_patterns = ['#', '/tag/', '/category/', '/author/', '/page/', 'login', 'register']
    
    for pattern in exclude_patterns:
        if pattern in url:
            return False
    
    return True


def _extract_content(url: str) -> Optional[str]:
    try:
        downloaded = trafilatura.fetch_url(url)
        
        if downloaded:
            content = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
            
            if content:
                return content.strip()
    except Exception as e:
        pass
    
    return None


def _extract_title_from_url(url: str) -> str:
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
    except Exception as e:
        pass
    
    return url


def _get_openai_client():
    api_key = os.getenv('LLM_API_KEY')
    base_url = os.getenv('LLM_BASE_URL')
    
    if not api_key:
        raise ValueError('LLM_API_KEY environment variable is not set')
    
    client_kwargs = {'api_key': api_key}
    if base_url:
        client_kwargs['base_url'] = base_url
    
    return OpenAI(**client_kwargs)


def _is_english(text: str) -> bool:
    if not text:
        return False
    
    english_chars = sum(1 for char in text if char.isalpha() and ord(char) < 128)
    total_chars = sum(1 for char in text if char.isalpha())
    
    if total_chars == 0:
        return False
    
    return english_chars / total_chars > 0.5


def _process_single_article_with_ai(client: OpenAI, article: Dict) -> Dict:
    title = article.get('title', '')
    content_text = article.get('content_text', '')
    url = article.get('url', '')
    
    is_english_title = _is_english(title)
    is_english_content = _is_english(content_text)
    
    prompt = f"""
    # Role
    ä½ æ˜¯ä¸€åèµ„æ·±çš„**æˆ·å¤–æé™è¿åŠ¨ç¼–è¾‘**ï¼Œç²¾é€šç™»å±±ï¼ˆAlpinismï¼‰ã€æ”€å²©ï¼ˆRock Climbingï¼‰ã€å¾’æ­¥ç­‰é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†å’Œæœ¯è¯­ã€‚ä½ çš„ä»»åŠ¡æ˜¯é˜…è¯»ä»¥ä¸‹æ–‡ç« ï¼Œæå–æ ¸å¿ƒä¿¡æ¯å¹¶ç”Ÿæˆå‘¨æŠ¥ç´ æã€‚

    # Input Data
    æ ‡é¢˜: {title}
    æ–‡ç« é“¾æ¥: {url}
    æ–‡ç« æ­£æ–‡: {content_text[:4000]} (é€‚å½“å¢åŠ é•¿åº¦ä»¥é˜²æˆªæ–­å…³é”®ä¿¡æ¯)

    # Goals
    è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ï¼š

    1. "chinese_title": 
    - å¦‚æœåŸæ–‡æ ‡é¢˜ä¸æ˜¯ä¸­æ–‡ï¼Œå°†æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚
    - **é‡è¦**ï¼šå¿…é¡»ä½¿ç”¨æˆ·å¤–åœˆä¸“ä¸šæœ¯è¯­ï¼ˆä¾‹å¦‚ï¼šFirst Ascentè¯‘ä¸º"é¦–æ”€"ï¼ŒFree Soloè¯‘ä¸º"æ— ä¿æŠ¤ç‹¬æ”€"ï¼ŒSendè¯‘ä¸º"å®Œæ”€"ï¼ŒPitchè¯‘ä¸º"ç»³è·"ï¼‰ã€‚
    - é£æ ¼è¦æ±‚ï¼šä¿¡è¾¾é›…ï¼Œåƒæ–°é—»æ ‡é¢˜ä¸€æ ·å¸å¼•äººã€‚

    2. "summary": 
    - ç”¨åŸæ–‡è¯­è¨€ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒäº‹ä»¶ã€‚
    - å¿…é¡»åŒ…å«ï¼šäººç‰© + åœ°ç‚¹ + å®Œæˆäº†ä»€ä¹ˆæˆå°±/å‘ç”Ÿäº†ä»€ä¹ˆäº‹æ•…ã€‚

    3. "chinese_summary": 
    - å¦‚æœsummaryä¸æ˜¯ä¸­æ–‡ï¼Œå°† summary ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¦åˆ™èµ‹å€¼summaryå³å¯
    - åŒæ ·è¦æ±‚ç²¾å‡†ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ã€‚

    4. "key_persons": 
    - æå–æ–‡ç« ä¸­çš„æ ¸å¿ƒäººç‰©å§“åï¼ˆä¿ç•™åŸåï¼Œä¸éœ€è¦ç¿»è¯‘ï¼‰ã€‚

    5. "location":
    - æå–äº‹ä»¶å‘ç”Ÿçš„åœ°ç‚¹ï¼ˆå¦‚ï¼šMount Everest, Yosemite, El Capitanï¼‰ã€‚å¦‚æœæœªæåŠï¼Œè¿”å› "æœªçŸ¥åœ°ç‚¹"ã€‚

    6. "event_date":
    - æå–äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´ï¼ˆå¦‚ï¼š2023å¹´10æœˆï¼Œæˆ–è€… Last weekï¼‰ã€‚å¦‚æœæœªæåŠï¼Œè¿”å›ä¸ºç©ºã€‚

    # Output Format
    å¿…é¡»è¿”å›çº¯å‡€çš„ JSON æ ¼å¼ï¼Œ**ä¸¥ç¦**ä½¿ç”¨ Markdown ä»£ç å—ï¼ˆå¦‚ ```json ... ```ï¼‰ï¼Œ**ä¸¥ç¦**è¾“å‡ºä»»ä½•å¼€åœºç™½æˆ–ç»“æŸè¯­ã€‚

    JSON ç»“æ„ç¤ºä¾‹ï¼š
    {{
    "chinese_title": "äºšå†å…‹æ–¯Â·éœè¯ºå¾·åœ¨çº¦å¡ç±³è’‚å®Œæˆå²è¯—çº§é¦–æ”€",
    "summary": "Alex Honnold completed the first solo ascent of...",
    "chinese_summary": "äºšå†å…‹æ–¯Â·éœè¯ºå¾·å®Œæˆäº†...",
    "key_persons": ["Alex Honnold"],
    "location": "El Capitan, Yosemite",
    "event_date": "2023-10-12"
    }}
    """

    model_name = os.getenv('LLM_MODEL')
    if not model_name:
        raise ValueError('LLM_MODEL environment variable is not set')

    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {'role': 'system', 'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æˆ·å¤–æ–°é—»æ–¹å‘çš„æ–‡ç« åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿æå–æ–‡ç« å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œä¸­è‹±æ–‡ç¿»è¯‘ã€‚'},
                {'role': 'user', 'content': prompt}
            ],
            temperature=0.3,
            response_format={'type': 'json_object'},
            timeout=30
        )
        
        result_text = response.choices[0].message.content.strip()
        
        import json
        result = json.loads(result_text)
        
        return {
            'original_title': title,
            'chinese_title': result.get('chinese_title', title),
            'summary': result.get('summary', content_text[:200] + '...'),
            'chinese_summary': result.get('chinese_summary', result.get('summary', content_text[:200] + '...')),
            'key_persons': result.get('key_persons', []),
            'url': url,
            'date': article.get('date', ''),
            'site': article.get('site', '')
        }
    except Exception as e:
        print(f"AIå¤„ç†å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
        return {
            'original_title': title,
            'chinese_title': title,
            'summary': content_text[:200] + '...',
            'chinese_summary': content_text[:200] + '...',
            'key_persons': [],
            'url': url,
            'date': article.get('date', ''),
            'site': article.get('site', '')
        }


def process_articles_with_ai(articles_list: List[Dict]) -> str:
    if not articles_list:
        return ''
    
    try:
        client = _get_openai_client()
    except Exception as e:
        print(f"åˆå§‹åŒ–AIå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        return ''
    
    processed_articles = []
    
    for i, article in enumerate(articles_list, 1):
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(articles_list)} ç¯‡æ–‡ç« ...")
        processed = _process_single_article_with_ai(client, article)
        processed_articles.append(processed)
    
    markdown_text = _generate_markdown(processed_articles)
    
    return markdown_text


def _generate_markdown(articles: List[Dict]) -> str:
    if not articles:
        return ''
    
    markdown_lines = []
    markdown_lines.append('# æˆ·å¤–è¿åŠ¨å‘¨æŠ¥\n')
    markdown_lines.append(f'ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n')
    markdown_lines.append(f'å…±æ”¶å½• {len(articles)} ç¯‡æ–‡ç« \n')
    markdown_lines.append('---\n')
    
    for i, article in enumerate(articles, 1):
        markdown_lines.append(f'\n## {i}. {article["chinese_title"]}\n')
        
        if article.get('original_title') and article.get('original_title') != article.get('chinese_title'):
            markdown_lines.append(f'**åŸæ ‡é¢˜**: {article["original_title"]}\n')
        
        if article.get('date'):
            markdown_lines.append(f'**æ—¥æœŸ**: {article["date"]}\n')
        
        markdown_lines.append(f'**é“¾æ¥**: {article["url"]}\n')
        
        if article.get('key_persons'):
            persons_text = 'ã€'.join(article['key_persons'])
            markdown_lines.append(f'**å…³é”®äººç‰©**: {persons_text}\n')
        
        markdown_lines.append(f'\n**æ‘˜è¦**: {article["summary"]}\n')
        
        if article.get('chinese_summary') and article.get('chinese_summary') != article.get('summary'):
            markdown_lines.append(f'\n*ä¸­æ–‡æ‘˜è¦*: {article["chinese_summary"]}\n')
        
        markdown_lines.append('\n---\n')
    
    return ''.join(markdown_lines)


def _parse_text_with_links(text):
    """
    [å†…éƒ¨å·¥å…·] è§£æåŒ…å« Markdown é“¾æ¥çš„æ–‡æœ¬
    è¾“å…¥: "ç‚¹å‡» [è¿™é‡Œ](http://google.com) æŸ¥çœ‹"
    è¾“å‡º: é£ä¹¦ TextElement ç»“æ„æ•°ç»„
    """
    elements = []
    # æ­£åˆ™åŒ¹é… [text](url)
    pattern = re.compile(r'\[(.*?)\]\((.*?)\)')
    last_idx = 0
    
    for match in pattern.finditer(text):
        # 1. æ·»åŠ é“¾æ¥å‰çš„æ™®é€šæ–‡æœ¬
        if match.start() > last_idx:
            elements.append(TextElement(
                text_run=TextRun(content=text[last_idx:match.start()])
            ))
        
        # 2. æ·»åŠ é“¾æ¥æ–‡æœ¬
        link_text = match.group(1)
        link_url = match.group(2)
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(link_text)
                .text_element_style(TextElementStyle.builder()
                    .link(Link.builder().url(link_url).build())
                    .build())
                .build())
            .build())
        last_idx = match.end()
    
    # 3. æ·»åŠ å‰©ä½™çš„æ–‡æœ¬
    if last_idx < len(text):
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(text[last_idx:])
                .build())
            .build())
        
    # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œç›´æ¥è¿”å›çº¯æ–‡æœ¬
    if not elements:
        elements.append(TextElement.builder()
            .text_run(TextRun.builder()
                .content(text)
                .build())
            .build())
        
    return elements

def publish_feishu_report(report_title, markdown_content, chat_id):
    """
    æ ¸å¿ƒåŠŸèƒ½: åˆ›å»ºæ–‡æ¡£ -> å†™å…¥å†…å®¹ -> å‘é€å¡ç‰‡
    """
    print(f"ğŸš€ [Feishu] å‡†å¤‡å‘å¸ƒæ–‡æ¡£: {report_title}")
    
    # =================================================
    # æ­¥éª¤ 1: åˆ›å»ºä¸€ä¸ªæ–°çš„ç©ºç™½æ–‡æ¡£
    # =================================================
    try:
        create_req = CreateDocumentRequest.builder() \
            .request_body(CreateDocumentRequestBody.builder()
                .title(report_title)
                .build()) \
            .build()
            
        resp = client.docx.v1.document.create(create_req)
        
        if not resp.success():
            print(f"âŒ åˆ›å»ºæ–‡æ¡£å¤±è´¥: {resp.code} - {resp.msg}")
            return None
            
        document_id = resp.data.document.document_id
        # æ³¨æ„: åªæœ‰é£ä¹¦å›½å†…ç‰ˆæ˜¯ feishu.cnï¼Œå›½é™…ç‰ˆè¯·æ”¹ä¸º larksuite.com
        doc_url = f"https://feishu.cn/docx/{document_id}"
        print(f"âœ… æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {doc_url}")

        collaborator_openids = os.getenv("FEISHU_COLLABORATOR_OPENIDS", "")
        collaborator_perm = os.getenv("FEISHU_COLLABORATOR_PERM", "edit")
        
        if collaborator_openids:
            openids = [oid.strip() for oid in collaborator_openids.split(",") if oid.strip()]
            
            added_count = 0
            failed_count = 0
            
            for openid in openids:
                try:
                    add_req = CreatePermissionMemberRequest.builder() \
                        .token(document_id) \
                        .type("docx") \
                        .need_notification(False) \
                        .request_body(BaseMember.builder()
                            .member_type("openid")
                            .member_id(openid)
                            .perm(collaborator_perm)
                            .perm_type("container")
                            .type("user")
                            .build()) \
                        .build()
                    
                    add_resp = client.drive.v1.permission_member.create(add_req)
                    
                    if add_resp.success():
                        print(f"âœ… åä½œè€…æ·»åŠ æˆåŠŸ: {openid}")
                        added_count += 1
                    else:
                        print(f"âš ï¸ åä½œè€…æ·»åŠ å¤±è´¥: {openid} - {add_resp.msg}")
                        failed_count += 1
                        
                except Exception as e:
                    print(f"âš ï¸ ä¸º {openid} æ·»åŠ åä½œè€…æ—¶å‡ºé”™: {e}")
                    failed_count += 1
            
            if added_count > 0:
                print(f"âœ… æˆåŠŸæ·»åŠ  {added_count} ä¸ªåä½œè€…ï¼Œæƒé™: {collaborator_perm}")
            if failed_count > 0:
                print(f"âš ï¸ {failed_count} ä¸ªåä½œè€…æ·»åŠ å¤±è´¥")

    except Exception as e:
        print(f"âŒ é£ä¹¦ API è¿æ¥é”™è¯¯: {e}")
        return None

    # =================================================
    # æ­¥éª¤ 2: ä½¿ç”¨é£ä¹¦å®˜æ–¹ API å°† Markdown è½¬æ¢ä¸º Blocks
    # =================================================
    print("ğŸ”„ æ­£åœ¨å°† Markdown è½¬æ¢ä¸ºé£ä¹¦æ–‡æ¡£å—...")
    
    # è°ƒç”¨é£ä¹¦å®˜æ–¹çš„ Markdown è½¬æ¢ API
    convert_req = ConvertDocumentRequest.builder() \
        .request_body(ConvertDocumentRequestBody.builder()
            .content_type("markdown")
            .content(markdown_content)
            .build()) \
        .build()
    
    convert_resp = client.docx.v1.document.convert(convert_req)
    
    if not convert_resp.success():
        print(f"âŒ Markdown è½¬æ¢å¤±è´¥: {convert_resp.code} - {convert_resp.msg}")
        return None
    
    # è·å–è½¬æ¢åçš„ blocks
    blocks = convert_resp.data.blocks
    first_level_block_ids = convert_resp.data.first_level_block_ids or []
    
    if not blocks:
        print("âš ï¸ è½¬æ¢åçš„å†…å®¹ä¸ºç©º")
        return doc_url
    
    # ä½¿ç”¨ first_level_block_ids é‡æ–°æ’åº blocks
    if first_level_block_ids:
        block_map = {b.block_id: b for b in blocks}
        ordered_blocks = []
        for block_id in first_level_block_ids:
            if block_id in block_map:
                ordered_blocks.append(block_map[block_id])
        # æ·»åŠ ä¸åœ¨ first_level_block_ids ä¸­çš„ blocks
        for block in blocks:
            if block.block_id not in first_level_block_ids:
                ordered_blocks.append(block)
        blocks = ordered_blocks
    
    print(f"âœ… Markdown è½¬æ¢æˆåŠŸï¼Œå…± {len(blocks)} ä¸ª blocks")
    
    # =================================================
    # æ­¥éª¤ 3: æ‰¹é‡å†™å…¥ blocks åˆ°æ–‡æ¡£
    # =================================================
    print("ğŸ“ æ­£åœ¨å†™å…¥æ–‡æ¡£å†…å®¹...")
    
    # æ‰¹é‡å†™å…¥ï¼ˆæ¯æ¬¡æœ€å¤š 100 ä¸ª blockï¼‰
    batch_size = 100
    for i in range(0, len(blocks), batch_size):
        batch_blocks = blocks[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        add_block_req = CreateDocumentBlockChildrenRequest.builder() \
            .document_id(document_id) \
            .block_id(document_id) \
            .request_body(CreateDocumentBlockChildrenRequestBody.builder()
                .children(batch_blocks)
                .build()) \
            .build()
        
        add_resp = client.docx.v1.document_block_children.create(add_block_req)
        
        if add_resp.success():
            print(f"âœ… æ‰¹æ¬¡ {batch_num} å†™å…¥æˆåŠŸ ({len(batch_blocks)} blocks)")
        else:
            print(f"âš ï¸ æ‰¹æ¬¡ {batch_num} å†™å…¥å¤±è´¥: {add_resp.code} - {add_resp.msg}")

    # =================================================
    # æ­¥éª¤ 4: å‘é€å¯Œæ–‡æœ¬å¡ç‰‡æ¶ˆæ¯
    # =================================================
    print(f"ğŸ“¤ æ­£åœ¨æ¨é€åˆ°ç¾¤ç»„: {chat_id}")
    
    # æ„é€ å¡ç‰‡ JSON
    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": "ğŸ§—â€â™‚ï¸ æˆ·å¤–èµ„è®¯å‘¨æŠ¥å·²ç”Ÿæˆ"},
            "template": "blue" # æ ‡é¢˜èƒŒæ™¯è‰²: blue, wathet, turquoise, green, yellow, orange, red, carmine, violet, purple, indigo, grey
        },
        "elements": [
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"æœ¬å‘¨èµ„è®¯å·²ç”± AI æ•´ç†å®Œæ¯•ã€‚\n**æ ‡é¢˜ï¼š** {report_title}\n**æ—¶é—´ï¼š** {os.getenv('TODAY', 'æœ¬å‘¨')}"
                }
            },
            {
                "tag": "hr" # åˆ†å‰²çº¿
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ‘‰ ç‚¹å‡»é˜…è¯»å®Œæ•´å‘¨æŠ¥"},
                        "url": doc_url,
                        "type": "primary"
                    }
                ]
            }
        ]
    }

    # å‘é€è¯·æ±‚
    msg_req = CreateMessageRequest.builder() \
        .receive_id_type("chat_id") \
        .request_body(CreateMessageRequestBody.builder() \
            .receive_id(chat_id) \
            .msg_type("interactive") \
            .content(json.dumps(card_content)) \
            .build()) \
        .build()

    msg_resp = client.im.v1.message.create(msg_req)
    
    if msg_resp.success():
        print("âœ… æ¶ˆæ¯æ¨é€æˆåŠŸ")
        return doc_url
    else:
        print(f"âŒ æ¶ˆæ¯æ¨é€å¤±è´¥: {msg_resp.code} - {msg_resp.msg}")
        return None
